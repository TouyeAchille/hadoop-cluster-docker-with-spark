name: hadoop-spark-cluster
version: '3'

services:
  hadoop-master:         # Hadoop namenode
    build: .             # Build the Dockerfile in the current directory 
    ports:
      - 9870:9870        # HDFS NameNode UI 
      - 8088:8088        # YARN ResourceManager UI
      - 8080:8080        # Spark Master UI 
      - 19888:19888      # YARN Job History Server       
      - 7077:7077        # Spark Master UI
      - 8888:8888        # Jupyter Notebook
    networks:
      - hadoop
    container_name: hadoop-master
    hostname: hadoop-master
    stdin_open: true
    tty: true
    ulimits:
      nofile:
        soft: 1024
        hard: 1024
    volumes:
      - notebooks:/root/notebooks
    command: sh -c "service ssh start && bash"

  hadoop-slave1:  # Hadoop datanode1
    build : .
    ports :
      - 8042:8042
    networks:
      - hadoop
    container_name: hadoop-slave1
    hostname: hadoop-slave1
    stdin_open: true
    tty: true
    ulimits:
      nofile:
        soft: 1024
        hard: 1024
    volumes:
      - notebooks:/root/notebooks    
    command: sh -c "service ssh start && bash"
    

  hadoop-slave2:
    build: .
    ports:
      - 8043:8042
    networks:
      - hadoop
    container_name: hadoop-slave2
    hostname: hadoop-slave2
    stdin_open: true
    tty: true
    ulimits:
      nofile:
        soft: 1024
        hard: 1024
    volumes:
      - notebooks:/root/notebooks    
    command: sh -c "service ssh start && bash"    

volumes:
  notebooks:
  
networks:
  hadoop:
    driver: bridge